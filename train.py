# ======================================================
# CheXpert Training Template (Production-ready)
# PyTorch Lightning + Weighted BCE + AUC + Scheduler
# ======================================================

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger

from src.dataset import CheXpertDataModule
from src.model import CheXpertLightning

# ======================================================
# CONFIG (‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠)
# ======================================================

DATA_DIR = "./data"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ CheXpert-v1.0-small
CSV_PATH = "./data/CheXpert-v1.0-small/train.csv"

IMG_SIZE = 320
BATCH_SIZE = 16
NUM_WORKERS = 4

MODEL_NAME = "densenet121"  # or efficientnet-b0
LR = 1e-4
EPOCHS = 15

# ‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏°‡∏≤
POS_WEIGHT = [5.68, 7.29, 14.01, 3.28, 1.59]

# ======================================================
# MAIN TRAINING
# ======================================================


def main():
    pl.seed_everything(42, workers=True)

    # -----------------------------
    # DataModule (üî• Stanford Policy)
    # -----------------------------
    dm = CheXpertDataModule(
        data_dir=DATA_DIR,
        csv_path=CSV_PATH,
        img_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        policy="custom",  # üî• Custom per-class policy with U-Ignore
    )

    # -----------------------------
    # Lightning Model
    # -----------------------------
    model = CheXpertLightning(
        model_name=MODEL_NAME,
        num_classes=5,
        lr=LR,
        pos_weight=POS_WEIGHT,  # ‚≠ê ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏∏‡∏î
    )

    # -----------------------------
    # Callbacks (‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
    # -----------------------------

    checkpoint = ModelCheckpoint(
        monitor="val_auc",
        mode="max",
        save_top_k=1,
        filename="best-{epoch:02d}-{val_auc:.4f}",
    )

    early_stop = EarlyStopping(
        monitor="val_auc",
        mode="max",
        patience=5,
    )

    lr_monitor = LearningRateMonitor(logging_interval="epoch")

    logger = TensorBoardLogger("logs", name="chexpert")

    # -----------------------------
    # Trainer
    # -----------------------------

    trainer = pl.Trainer(
        max_epochs=EPOCHS,
        accelerator="auto",
        devices="auto",
        precision="16-mixed",  # ‚≠ê ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ~1.7x
        callbacks=[checkpoint, early_stop, lr_monitor],
        logger=logger,
        log_every_n_steps=20,
    )

    # -----------------------------
    # Train
    # -----------------------------

    trainer.fit(model, dm)

    print("\nBest checkpoint:", checkpoint.best_model_path)


# ======================================================

if __name__ == "__main__":
    main()
